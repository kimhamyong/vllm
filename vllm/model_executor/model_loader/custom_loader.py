# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project

import collections
import glob
import os
from collections.abc import Generator
from typing import Any, Optional

import torch
from torch import nn

from vllm.config import LoadConfig, ModelConfig
from vllm.logger import init_logger
from vllm.model_executor.model_loader.base_loader import BaseModelLoader
from vllm.model_executor.model_loader.weight_utils import (
    download_weights_from_hf, runai_safetensors_weights_iterator)
from vllm.transformers_utils.s3_utils import glob as s3_glob
from vllm.transformers_utils.utils import is_s3
from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy
import shutil
import ray

logger = init_logger(__name__)
ENABLE_LOAD_LOG = True

class CustomLoader(BaseModelLoader):
    """
    Custom model loader (based on ShardedStateLoader).
    """

    DEFAULT_PATTERN = "model-rank-{rank}-part-{part}.safetensors"

    def __init__(self,
                 load_config: LoadConfig, # ëª¨ë¸ ë¡œë”©ì— í•„ìš”í•œ ì„¤ì • ì •ë³´
                 runai_model_streamer: bool = False):
        super().__init__(load_config) # load_configë¥¼ ìƒìœ„ í´ë˜ìŠ¤ì— ë„˜ê²¨ì£¼ê³  ì´ˆê¸°í™” ì‘ì—…ì„ ìœ„ì„

        self.runai_model_streamer = runai_model_streamer
        extra_config = ({} if load_config.model_loader_extra_config is None
                        else load_config.model_loader_extra_config.copy())
        self.pattern = extra_config.pop("pattern", self.DEFAULT_PATTERN)
        if extra_config:
            raise ValueError(f"Unexpected extra config keys for load format "
                             f"{load_config.load_format}: "
                             f"{load_config.model_loader_extra_config.keys()}")


#-----------------------------------------------------------------------------------
    @staticmethod
    def _filter_subtensors(
        tensors: dict[str, torch.Tensor], ) -> dict[str, torch.Tensor]:
        same_storage_groups: dict[Any, list[tuple[str, torch.Tensor]]] = (
            collections.defaultdict(list))
        for key, tensor in tensors.items():
            if tensor.numel():
                ptr = tensor.untyped_storage().data_ptr()
                same_storage_groups[tensor.device, ptr].append((key, tensor))

        def get_end_ptr(tensor: torch.Tensor) -> int:
            return tensor.view(-1)[-1].data_ptr() + tensor.element_size()

        result: dict[str, torch.Tensor] = {}
        for group in same_storage_groups.values():
            for k, t in group:
                a, b = t.data_ptr(), get_end_ptr(t)
                for k2, t2 in group:
                    if not t2.is_contiguous():
                        continue
                    a2, b2 = t2.data_ptr(), get_end_ptr(t2)
                    if a < a2 or b2 < b:
                        continue
                    if a2 < a or b < b2 or not t.is_contiguous():
                        break
                    if k2 < k:
                        break
                else:
                    result[k] = t
        return result
#-----------------------------------------------------------------------------------
    def _prepare_weights(self, model_name_or_path: str,
                         revision: Optional[str]):
        if is_s3(model_name_or_path) or os.path.isdir(model_name_or_path):
            return model_name_or_path
        else:
            allow_patterns = ["*.safetensors"]
            return download_weights_from_hf(
                model_name_or_path,
                self.load_config.download_dir,
                allow_patterns,
                revision,
                ignore_patterns=self.load_config.ignore_patterns,
            )


#-----------------------------------------------------------------------------------
    def download_model(self, model_config: ModelConfig) -> None:
        self._prepare_weights(model_config.model, model_config.revision)

#-----------------------------------------------------------------------------------
    @staticmethod
    @ray.remote(num_cpus=0)
    def _pull_files(dir_root: str, tag: str, pattern: str):
        import glob, os, socket

        ip   = socket.gethostbyname(socket.gethostname())   # ì‹¤í–‰ ë…¸ë“œ IP
        patt = os.path.join(dir_root, pattern.format(rank=tag, part="*"))
        files = [(os.path.basename(fp), open(fp, "rb").read())
                for fp in glob.glob(patt)]
        if files:
            print(f"âœ…[Ray {ip}] {len(files)} file(s) matched {patt}")
        else:
            print(f"âŒ[Ray {ip}] no file for {patt}")
        return {"ip": ip, "files": files}

#-----------------------------------------------------------------------------------
    def load_weights(self, model: nn.Module,
                     model_config: ModelConfig) -> None:
        from vllm.distributed import get_tensor_model_parallel_rank
        from vllm.distributed import get_tensor_model_parallel_world_size
        import torch
        import ray, tempfile, os, glob, shutil
        from safetensors.torch import safe_open

        print("[ğŸ‘Œ] CustomLoader loading")

        model_weights = getattr(model_config, "model_weights", model_config.model)
        local_model_path = model_weights

        # í˜„ì¬ TP rank ë° world size
        rank       = get_tensor_model_parallel_rank()
        world_size = get_tensor_model_parallel_world_size()
        half       = world_size // 2

        # â”€â”€ rankë³„ë¡œ ê°€ì ¸ì˜¬ shard íƒœê·¸ ê²°ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if rank < half:               # rank 0~(half-1)
            desired_tags = (
                f"{rank}0",           # ìê¸° 0ë²ˆ
                f"{rank}1",           # ìê¸° 1ë²ˆ
                f"{rank+half}0",      # ë’· ë…¸ë“œì˜ 0ë²ˆ
            )
        else:                         # rank â‰¥ half
            desired_tags = (f"{rank}1",)  # ìê¸° 1ë²ˆë§Œ
        print(f"ğŸ…¾ï¸[Rank {rank}] Desired tags: {desired_tags}")

        # ë¡œì»¬ì—ì„œ shard íŒŒì¼ íƒìƒ‰
        filepaths, missing_tags = [], []
        for tag in desired_tags:
            pattern = os.path.join(local_model_path,
                                self.pattern.format(rank=tag, part="*"))
            found = glob.glob(pattern)
            print(f"ğŸ”µ[Rank {rank}] Local Tag {tag} found files: {found}")
            if found:
                filepaths += found
                print(f"ğŸ…°ï¸[Rank {rank}] Tag {tag} total files: {len(filepaths)}")
            else:
                missing_tags.append(tag)
                print(f"ğŸ…±ï¸[Rank {rank}] missing_tags {tag}")

        # ë¶€ì¡±í•œ íƒœê·¸ê°€ ìˆìœ¼ë©´ ì›ê²© ë…¸ë“œì—ì„œ ìˆ˜ì§‘
        if missing_tags:
            pulled = []
            for tag in missing_tags:
                print(f"ğŸ˜Š[Rank {rank}] Searching tag {tag} on every node")
                futures = [
                    CustomLoader._pull_files.options(
                        placement_group=None,
                        num_cpus=0,
                        scheduling_strategy=NodeAffinitySchedulingStrategy(
                            node_id=n["NodeID"], soft=True),
                    ).remote(local_model_path, tag, self.pattern)
                    for n in ray.nodes()
                ]
                # ëª¨ë“  ë…¸ë“œì—ì„œ ê²°ê³¼ ì·¨í•©
                for res in ray.get(futures):
                    pulled.extend(res["files"])

            # pulled íŒŒì¼ì„ ì„ì‹œ ë””ë ‰í„°ë¦¬ë¡œ ì €ì¥
            if pulled:
                tmp_dir = tempfile.mkdtemp(prefix=f"remote_ckpt_rank{rank}_")
                print(f"âœ…[Rank {rank}] Saving pulled files to tmp_dir={tmp_dir}")
                for name, raw in pulled:
                    path = os.path.join(tmp_dir, name)
                    if path in filepaths:          # ì¤‘ë³µ ë°©ì§€
                        continue
                    with open(path, "wb") as f:
                        f.write(raw)
                    filepaths.append(path)

        def _collect_available_keys(paths):
            keys = set()
            for fp in paths:
                with safe_open(fp, framework="pt", device="cpu") as f:
                    keys |= set(f.keys())
            return keys
        available_keys = _collect_available_keys(filepaths)

        if not filepaths:
            raise ValueError(f"âŒ[Rank {rank}] No shard files found")


        if not filepaths:
            # TODO: support un-sharded checkpoints too
            raise ValueError(
                f"Could not find checkpoint files '{pattern}', only "
                f"pre-sharded checkpoints are currently supported!")

        state_dict = {
            k: v
            for k, v in self._filter_subtensors(model.state_dict()).items()
            if k in available_keys          # part-0 ìª½ keyë§Œ ìœ ì§€
        }

        # ëª¨ë¸ ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = 0

        temp_parts    = {} # half-shard buffer

        # shard íŒŒì¼ ìˆœíšŒí•˜ë©° ë¡œë“œ
        for key, tensor in self.iterate_over_files(filepaths, rank):

            loaded_params = tensor.numel()
            total_params += loaded_params

            # state_dictì— í‚¤ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ    
            if key not in state_dict:
                continue

            # ë‘ íŒŒì¼ì„ í•©ì³ì„œ ë¡œë“œ -> ë‚˜ëˆ ì§„ shard íŒŒì¼ì„ í•©ì¹˜ëŠ” ê²½ìš°
            # `lm_head.weight`ëŠ” ë‘ íŒŒì¼ì— ë™ì¼í•˜ê²Œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸ í…ì„œë§Œ ì‚¬ìš©
            if tensor.shape != state_dict[key].shape and key != "lm_head.weight": # í…ì„œì˜ shapeì´ `state_dict`ì— ì´ë¯¸ ì €ì¥ëœ íŒŒë¼ë¯¸í„°ì™€ ë‹¤ë¥´ë©´
                buf = temp_parts.setdefault(key, tensor)
                if buf is tensor: # ì²« ë²ˆì§¸ shardê°€ ì´ë¯¸ ì €ì¥ë˜ì–´ `key`ê°€ `temp_parts`ì— ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë‹¤ìŒ shardê°€ ì˜¬ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
                    continue
                tensor = torch.cat([buf, tensor], dim=-1) # `torch.cat()`ì„ ì‚¬ìš©í•˜ì—¬ ë‘ í…ì„œë¥¼ í•©ì¹¨
                temp_parts.pop(key) # ë‘ ê°œì˜ shardê°€ í•©ì³ì§€ë©´, `temp_parts`ì—ì„œ í•´ë‹¹ `key` ì‚­ì œ

            # tensor â†’ param ë³µì‚¬ 
            dst = state_dict[key].data
            for dim, size in enumerate(tensor.shape):
                if size < dst.shape[dim]:
                    dst = dst.narrow(dim, 0, size)
            dst.copy_(tensor)

            state_dict.pop(key)

        # ë¡œë”© ë¡œê·¸
        logger.info(f"âœ”ï¸âœ”ï¸[Rank {rank}] Total Loaded Parameters Across All Files: {total_params:,}")

        if state_dict:   # ë‚¨ì€ key = part-1 ì˜ì—­
            logger.info(
                "ğŸ”µ[Rank %d] %d keys skipped (partial-load TP): %s",
                rank, len(state_dict), list(state_dict)[:5]
            )

#-----------------------------------------------------------------------------------
    def iterate_over_files(
            self, paths, rank) -> Generator[tuple[str, torch.Tensor], None, None]:
        if self.runai_model_streamer:
            yield from runai_safetensors_weights_iterator(paths, True)
        else:
            from safetensors.torch import safe_open

            for path in paths:
                logger.info(f"â˜‘ï¸[Rank {rank}] Trying to open file: {path}")

                total_file_params = 0
                with safe_open(path, framework="pt") as f:

                    for key in f.keys():  # noqa: SIM118
                        tensor = f.get_tensor(key)
                        total_file_params += tensor.numel()

                        yield key, tensor

                logger.info(f"âœ”ï¸[Rank {rank}] Loaded {total_file_params:,} parameters from file {path}")

#-----------------------------------------------------------------------------------
# ê¸°ì¡´ ì½”ë“œ:
# í˜„ì¬ rankê°€ ê°€ì§„ weight(state_dict)ë§Œ ê°€ì ¸ì˜´
# ê° í…ì„œ í¬ê¸°ë¥¼ í™•ì¸í•´ì„œ max_size ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
# ë¶„í• ëœ íŒŒì¼ë“¤ì„ {rank}-{part} í˜•íƒœë¡œ safetensorsë¡œ ì €ì¥

# ë³€ê²½ ì½”ë“œ:
# state_dictì˜ ê° key(íŒŒë¼ë¯¸í„°)ë³„ í…ì„œë¥¼ 1/2ë¡œ ë‚˜ëˆ„ì–´
#     â””â”€ ì²« ì ˆë°˜ â†’ rankX0, ë‘ ë²ˆì§¸ ì ˆë°˜ â†’ rankX1 íŒŒì¼ì— ì €ì¥


    @staticmethod
    def save_model(
        # model ìì²´ê°€ í˜„ì¬ rank í”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‘ ì¤‘ì¸ TP ë¶„í•  ëª¨ë¸
        model: torch.nn.Module, # í˜„ì¬ rankì˜ TP ë¶„í•  ëª¨ë¸ (í•„ìˆ˜)
        path: str, # ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ (í•„ìˆ˜)
        pattern: Optional[str] = None, # ì €ì¥ íŒŒì¼ ì´ë¦„ íŒ¨í„´ -> ìœ„ì—ì„œ ì •ì˜í•¨
        max_size: Optional[int] = None, # ê° íŒŒì¼ì˜ ìµœëŒ€ í¬ê¸°(ë°”ì´íŠ¸), ì´ˆê³¼ ì‹œ ì—¬ëŸ¬ íŒŒì¼ë¡œ ìª¼ê°¬
    ) -> None:
        from safetensors.torch import save_file
        from vllm.distributed import get_tensor_model_parallel_rank
        import torch

        if pattern is None:
            pattern = CustomLoader.DEFAULT_PATTERN

        rank = get_tensor_model_parallel_rank() # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ rank, ì €ì¥ íŒŒì¼ì— ì´ rank ê°’ì´ ì‚¬ìš©ë¨ -> íŒŒì¼ ë¶„ë¦¬ ê¸°ì¤€ì„

        # ì €ì¥ ë°˜ë³µ êµ¬ì¡° ì´ˆê¸°í™”
        part_idx = 0 # í•˜ë‚˜ì˜ rankê°€ ì €ì¥í•˜ëŠ” íŒŒì¼ ë²ˆí˜¸
        total_size = 0 # í˜„ì¬ íŒŒì¼ì— ëˆ„ì ëœ íŒŒë¼ë¯¸í„° í¬ê¸°

        # state_dict()ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì˜´ -> CustomLoader._filter_subtensors()ë¡œ í•„í„°ë§: ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œì™¸
        state_dict = CustomLoader._filter_subtensors(model.state_dict())

        # ì €ì¥ ë°˜ë³µ êµ¬ì¡° ì´ˆê¸°í™”: í˜„ì¬ íŒŒì¼ì— ì €ì¥í•  key-value í…ì„œ ë”•ì…”ë„ˆë¦¬ (2ê°œë¡œ ë¶„í• )
        state_dict_part_0: dict[str, torch.Tensor] = {}  # ì²« ë²ˆì§¸ ì ˆë°˜
        state_dict_part_1: dict[str, torch.Tensor] = {}  # ë‘ ë²ˆì§¸ ì ˆë°˜
        print("[ğŸ‘ŒğŸ‘Œ] CustomLoader save_model")

        # state_dict ìˆœíšŒí•˜ë©´ì„œ íŒŒì¼ ë¶„í•  ì €ì¥
        for key, tensor in state_dict.items():

            if key == "lm_head.weight":
                # ë‘ íŒŒì¼(0Â·1) ëª¨ë‘ì— ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
                state_dict_part_0[key] = tensor.contiguous()
                state_dict_part_1[key] = tensor.contiguous()
                param_size = tensor.nelement() * tensor.element_size()
                total_size += param_size
                continue

                if not logged_lm_head:          # í•œ rankì—ì„œ 1ë²ˆë§Œ ì¶œë ¥
                    print(f"[CHECK][rank {rank}] lm_head.weight duplicated "
                        f"â†’ will be in {rank}0 / {rank}1")
                    logged_lm_head = True
                continue

            # ê° í…ì„œë¥¼ 2ë“±ë¶„
            if len(tensor.shape) >= 1 and tensor.shape[-1] >= 2:
                # torch.splitì„ ì‚¬ìš©í•˜ì—¬ contiguousí•œ í…ì„œ ìƒì„±
                split_size = tensor.shape[-1] // 2
                remaining_size = tensor.shape[-1] - split_size
            
                # ì²« ë²ˆì§¸ ì ˆë°˜ê³¼ ë‘ ë²ˆì§¸ ì ˆë°˜ìœ¼ë¡œ ë¶„í• 
                first_half, second_half = torch.split(tensor, [split_size, remaining_size], dim=-1)
                first_half = first_half.contiguous()
                second_half = second_half.contiguous()
            
                # í…ì„œê°€ ëª‡ ë°”ì´íŠ¸ë¥¼ ì°¨ì§€í•˜ëŠ”ì§€ ê³„ì‚°
                param_size_0 = first_half.nelement() * first_half.element_size()
                param_size_1 = second_half.nelement() * second_half.element_size()
            
                # ì €ì¥í•  í…ì„œë¥¼ ì¶”ê°€í–ˆì„ ë•Œ, ì„¤ì •ëœ ìµœëŒ€ íŒŒì¼ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
                if max_size is not None and total_size + param_size_0 > max_size: # total_size: ëˆ„ì  í¬ê¸°
                
                    # ì €ì¥í•  íŒŒì¼ ì´ë¦„ì„ ìƒì„±
                    filename_0 = pattern.format(rank=f"{rank}0", part=part_idx)
                    filename_1 = pattern.format(rank=f"{rank}1", part=part_idx)
                
                    # ë”•ì…”ë„ˆë¦¬ {key: tensor}ë¥¼ .safetensors íŒŒì¼ë¡œ ì €ì¥
                    save_file( # ì§€ê¸ˆê¹Œì§€ ëª¨ì•„ë‘” íŒŒë¼ë¯¸í„° ë¬¶ìŒ (ì²« ë²ˆì§¸ ì ˆë°˜)
                        {k: v.contiguous() for k, v in state_dict_part_0.items()}, # contiguousë¡œ ë³€í™˜
                        os.path.join(path, filename_0),
                    )
                    save_file( # ì§€ê¸ˆê¹Œì§€ ëª¨ì•„ë‘” íŒŒë¼ë¯¸í„° ë¬¶ìŒ (ë‘ ë²ˆì§¸ ì ˆë°˜)
                        {k: v.contiguous() for k, v in state_dict_part_1.items()}, # contiguousë¡œ ë³€í™˜
                        os.path.join(path, filename_1),
                    )
                    print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œ] {rank}, {filename_0}")
                    print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œ] {rank}, {filename_1}")

                    # ë‹¤ìŒì— ì €ì¥í•  íŒŒì¼ì˜ part ë²ˆí˜¸ë¥¼ 1 ì¦ê°€ -> í•˜ë‚˜ì˜ rankê°€ ì—¬ëŸ¬ íŒŒì¼ì„ ì €ì¥
                    part_idx += 1
                    # ëˆ„ì  í¬ê¸°ë¥¼ ì´ˆê¸°í™”í•´ì„œ ë‹¤ìŒ íŒŒì¼ì— í…ì„œë“¤ì„ ë‹¤ì‹œ ì±„ìš°ê¸° ì‹œì‘
                    total_size = 0
                    # save_fileë¡œ ì €ì¥í•œ í…ì„œ ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ˆê¸°í™”
                    state_dict_part_0 = {}
                    state_dict_part_1 = {}

                # í˜„ì¬ ìˆœíšŒ ì¤‘ì¸ í…ì„œì˜ ì ˆë°˜ë“¤ì„ ê°ê° state_dict_partì— ì¶”ê°€
                state_dict_part_0[key] = first_half
                state_dict_part_1[key] = second_half
                # ì´ í…ì„œì˜ ë°”ì´íŠ¸ ìˆ˜ë¥¼ í˜„ì¬ íŒŒì¼ì— ëˆ„ì ëœ ìš©ëŸ‰ì— ë”í•¨
                total_size += param_size_0
            
            else:
                # ë¶„í• í•  ìˆ˜ ì—†ëŠ” í…ì„œëŠ” ë³µì œ
                tensor_contiguous = tensor.contiguous()
                param_size = tensor_contiguous.nelement() * tensor_contiguous.element_size()
            
                # ì €ì¥í•  í…ì„œë¥¼ ì¶”ê°€í–ˆì„ ë•Œ, ì„¤ì •ëœ ìµœëŒ€ íŒŒì¼ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
                if max_size is not None and total_size + param_size > max_size: # total_size: ëˆ„ì  í¬ê¸°
                
                    # ì €ì¥í•  íŒŒì¼ ì´ë¦„ì„ ìƒì„±
                    filename_0 = pattern.format(rank=f"{rank}0", part=part_idx)
                    filename_1 = pattern.format(rank=f"{rank}1", part=part_idx)
                
                    # ë”•ì…”ë„ˆë¦¬ {key: tensor}ë¥¼ .safetensors íŒŒì¼ë¡œ ì €ì¥
                    save_file(
                        state_dict_part_0, # ì§€ê¸ˆê¹Œì§€ ëª¨ì•„ë‘” íŒŒë¼ë¯¸í„° ë¬¶ìŒ (ì²« ë²ˆì§¸ ì ˆë°˜)
                        os.path.join(path, filename_0),
                    )
                    save_file(
                        state_dict_part_1, # ì§€ê¸ˆê¹Œì§€ ëª¨ì•„ë‘” íŒŒë¼ë¯¸í„° ë¬¶ìŒ (ë‘ ë²ˆì§¸ ì ˆë°˜)
                        os.path.join(path, filename_1),
                    )
                    print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œ] {rank}, {filename_0}")
                    print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œ] {rank}, {filename_1}")

                    # ë‹¤ìŒì— ì €ì¥í•  íŒŒì¼ì˜ part ë²ˆí˜¸ë¥¼ 1 ì¦ê°€ -> í•˜ë‚˜ì˜ rankê°€ ì—¬ëŸ¬ íŒŒì¼ì„ ì €ì¥
                    part_idx += 1
                    # ëˆ„ì  í¬ê¸°ë¥¼ ì´ˆê¸°í™”í•´ì„œ ë‹¤ìŒ íŒŒì¼ì— í…ì„œë“¤ì„ ë‹¤ì‹œ ì±„ìš°ê¸° ì‹œì‘
                    total_size = 0
                    # save_fileë¡œ ì €ì¥í•œ í…ì„œ ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ˆê¸°í™”
                    state_dict_part_0 = {}
                    state_dict_part_1 = {}

                # í˜„ì¬ ìˆœíšŒ ì¤‘ì¸ í…ì„œë¥¼ ì–‘ìª½ ëª¨ë‘ì— ì¶”ê°€ (ë³µì œ)
                state_dict_part_0[key] = tensor_contiguous
                state_dict_part_1[key] = tensor_contiguous
                # ì´ í…ì„œì˜ ë°”ì´íŠ¸ ìˆ˜ë¥¼ í˜„ì¬ íŒŒì¼ì— ëˆ„ì ëœ ìš©ëŸ‰ì— ë”í•¨
                total_size += param_size

        # ë°˜ë³µì´ ëë‚œ í›„, ì €ì¥ë˜ì§€ ì•Šì€ ë§ˆì§€ë§‰ ë¬¶ìŒì„ ì €ì¥   
        if len(state_dict_part_0) > 0:
            filename_0 = pattern.format(rank=f"{rank}0", part=part_idx)
            filename_1 = pattern.format(rank=f"{rank}1", part=part_idx)
            save_file(
                state_dict_part_0,
                os.path.join(path, filename_0),
            )
            save_file(
                state_dict_part_1,
                os.path.join(path, filename_1),
            )
            print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œâœ…] {rank}, {filename_0}")
            print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œâœ…] {rank}, {filename_1}")

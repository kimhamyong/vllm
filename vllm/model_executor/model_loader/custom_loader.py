# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project

import collections
import glob
import os
from collections.abc import Generator
from typing import Any, Optional

import torch
from torch import nn

from vllm.config import LoadConfig, ModelConfig
from vllm.logger import init_logger
from vllm.model_executor.model_loader.base_loader import BaseModelLoader
from vllm.model_executor.model_loader.weight_utils import (
    download_weights_from_hf, runai_safetensors_weights_iterator)
from vllm.transformers_utils.s3_utils import glob as s3_glob
from vllm.transformers_utils.utils import is_s3
from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy
import shutil

logger = init_logger(__name__)

class CustomLoader(BaseModelLoader):
    """
    Custom model loader (based on ShardedStateLoader).
    """

    DEFAULT_PATTERN = "model-rank-{rank}-part-{part}.safetensors"

    def __init__(self,
                 load_config: LoadConfig, # ëª¨ë¸ ë¡œë”©ì— í•„ìš”í•œ ì„¤ì • ì •ë³´
                 runai_model_streamer: bool = False):
        super().__init__(load_config) # load_configë¥¼ ìƒìœ„ í´ë˜ìŠ¤ì— ë„˜ê²¨ì£¼ê³  ì´ˆê¸°í™” ì‘ì—…ì„ ìœ„ì„

        self.runai_model_streamer = runai_model_streamer
        extra_config = ({} if load_config.model_loader_extra_config is None
                        else load_config.model_loader_extra_config.copy())
        self.pattern = extra_config.pop("pattern", self.DEFAULT_PATTERN)
        if extra_config:
            raise ValueError(f"Unexpected extra config keys for load format "
                             f"{load_config.load_format}: "
                             f"{load_config.model_loader_extra_config.keys()}")


#-----------------------------------------------------------------------------------
    @staticmethod
    def _filter_subtensors(
        tensors: dict[str, torch.Tensor], ) -> dict[str, torch.Tensor]:
        same_storage_groups: dict[Any, list[tuple[str, torch.Tensor]]] = (
            collections.defaultdict(list))
        for key, tensor in tensors.items():
            if tensor.numel():
                ptr = tensor.untyped_storage().data_ptr()
                same_storage_groups[tensor.device, ptr].append((key, tensor))

        def get_end_ptr(tensor: torch.Tensor) -> int:
            return tensor.view(-1)[-1].data_ptr() + tensor.element_size()

        result: dict[str, torch.Tensor] = {}
        for group in same_storage_groups.values():
            for k, t in group:
                a, b = t.data_ptr(), get_end_ptr(t)
                for k2, t2 in group:
                    if not t2.is_contiguous():
                        continue
                    a2, b2 = t2.data_ptr(), get_end_ptr(t2)
                    if a < a2 or b2 < b:
                        continue
                    if a2 < a or b < b2 or not t.is_contiguous():
                        break
                    if k2 < k:
                        break
                else:
                    result[k] = t
        return result
#-----------------------------------------------------------------------------------
    def _prepare_weights(self, model_name_or_path: str,
                         revision: Optional[str]):
        if is_s3(model_name_or_path) or os.path.isdir(model_name_or_path):
            return model_name_or_path
        else:
            allow_patterns = ["*.safetensors"]
            return download_weights_from_hf(
                model_name_or_path,
                self.load_config.download_dir,
                allow_patterns,
                revision,
                ignore_patterns=self.load_config.ignore_patterns,
            )


#-----------------------------------------------------------------------------------
    def download_model(self, model_config: ModelConfig) -> None:
        self._prepare_weights(model_config.model, model_config.revision)


#-----------------------------------------------------------------------------------
    def load_weights(self, model: nn.Module,
                     model_config: ModelConfig) -> None:
        from vllm.distributed import get_tensor_model_parallel_rank
        from vllm.distributed import get_tensor_model_parallel_world_size
        import torch
        import ray, tempfile, os, glob, shutil

        model_weights = model_config.model

        print("[ğŸ‘Œ] CustomLoader loading")

        if hasattr(model_config, "model_weights"):
            model_weights = model_config.model_weights
        local_model_path = model_weights

        # í˜„ì¬ TP í™˜ê²½ì—ì„œì˜ rankë¥¼ ê°€ì ¸ì˜´
        rank = get_tensor_model_parallel_rank()

        # í˜„ì¬ TP rank / world í¬ê¸°
        world_size = get_tensor_model_parallel_world_size()
        half = world_size // 2 # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ì¼ë‹¨ ë‹¨ìˆœíˆ ë” ìª¼ê°  í¬ê¸°ë§Œí¼ ë‚˜ëˆ”â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # â”€â”€ rankë³„ë¡œ ê°€ì ¸ì˜¬ shard íƒœê·¸ ê²°ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if rank < half:                                   # rank 0~(half-1)
            desired_tags = (
                f"{rank}0",                               # ìê¸° 0ë²ˆ
                f"{rank}1",                               # ìê¸° 1ë²ˆ
                f"{rank + half}0",                        # ë’·ë…¸ë“œì˜ 0ë²ˆ (
            )
        else:                                             # rank â‰¥ half
            desired_tags = (
                f"{rank}1",                               # ìê¸° 1ë²ˆë§Œ
            )

        print(f"ğŸ…¾ï¸[Rank {rank}] Desired tags: {desired_tags}")

        filepaths = []
        # ë¡œì»¬ì— ì—†ëŠ” shard(tag) ì°¾ê¸° â†’ Rayë¡œ ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ê°€ì ¸ì˜¤ë„ë¡
        missing_tags = []

        for tag in desired_tags:
            pattern = os.path.join(
                local_model_path,
                self.pattern.format(rank=tag, part="*"),
            )
            found = glob.glob(pattern)          # ì´ë¯¸ í•œ ë²ˆ ì“´ ì½”ë“œ ì¬ì‚¬ìš©
            print(f"ğŸ”µ[Rank {rank}] Tag {tag} filepaths: {found}")
            filepaths += found                  # ìˆìœ¼ë©´ filepaths ì— ì¶”ê°€
            print(f"ğŸ”½[Rank {rank}] Tag {tag} found files: {filepaths}")
            if not found:                       # ì—†ìœ¼ë©´ missing
                missing_tags.append(tag)

        if missing_tags:
            @ray.remote(num_cpus=0)
            def _pull_files(dir_root: str, tag: str, pattern: str):
                import glob, os, socket

                ip   = socket.gethostbyname(socket.gethostname())   # ì‹¤í–‰ ë…¸ë“œ IP
                patt = os.path.join(dir_root, pattern.format(rank=tag, part="*"))
                files = [(os.path.basename(fp), open(fp, "rb").read())
                        for fp in glob.glob(patt)]
                if files:
                    print(f"âœ…[Ray {ip}] {len(files)} file(s) matched {patt}")
                else:
                    print(f"âŒ[Ray {ip}] no file for {patt}")
                return {"ip": ip, "files": files}
     
            pulled = []
            for tag in missing_tags:
                print(f"ğŸ…¾ï¸[Rank {rank}] Searching tag {tag} on every node")

                futures = [
                    _pull_files.options(
                        placement_group=None,
                        num_cpus=0,
                        scheduling_strategy=NodeAffinitySchedulingStrategy(
                           node_id=n["NodeID"],
                           soft=True,
                        ),
                    ).remote(local_model_path, tag, self.pattern)
                    for n in ray.nodes()
                ]

                results = ray.get(futures)
                found_any = False
                for res in results:
                    ip    = res["ip"]
                    files = res["files"]
                    if files:
                        names = [n for n, _ in files]
                        print(f"ğŸŒ[node {ip}] FOUND {names}")
                        pulled.extend(files)
                        found_any = True
                    else:
                        print(f"ğŸŒ[node {ip}] no file")
                if not found_any:
                    print(f"âŒ[Rank {rank}] Tag {tag}: not found on ANY node")
     
            if pulled:
                tmp_dir = tempfile.mkdtemp(prefix=f"remote_ckpt_rank{rank}_")
                print(f"âœ…[Rank {rank}] Saving pulled files to tmp_dir={tmp_dir}")

                for name, raw in pulled:
                    tmp_path = os.path.join(tmp_dir, name)

                    # ë™ì¼ íŒŒì¼ ì¤‘ë³µ ë°©ì§€
                    if tmp_path in filepaths:
                        print(f"âŒ[Rank {rank}] Duplicate {name} skipped")
                        continue

                    with open(tmp_path, "wb") as f:
                        f.write(raw)
                    filepaths.append(tmp_path)
                    print(f"âœ…[Rank {rank}] Saved: {name}")
                    print(f"ğŸ”½[Rank {rank}] files: {filepaths}")
                
                # ë¡œë“œê°€ ëë‚œ ë’¤ ì„ì‹œ ë””ë ‰í„°ë¦¬ ì‚­ì œ
                # shutil.rmtree(tmp_dir, ignore_errors=True)

        if not filepaths:
            # TODO: support un-sharded checkpoints too
            raise ValueError(
                f"Could not find checkpoint files '{pattern}', only "
                f"pre-sharded checkpoints are currently supported!")
        state_dict = self._filter_subtensors(model.state_dict())

        # ëª¨ë¸ ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(param.numel() for param in state_dict.values())

        temp_parts = {}   # ì²« ë²ˆì§¸ ì ˆë°˜ ë³´ê´€ìš©

        loaded_params = 0 

        # ê° íŒŒì¼ì„ ìˆœíšŒí•˜ë©´ì„œ ë¶„í• ëœ tensorë¥¼ êº¼ëƒ„
        for key, tensor in self.iterate_over_files(filepaths, rank):

            # state_dictì— í‚¤ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ    
            if key not in state_dict:
                continue

            # ë‘ íŒŒì¼ì„ í•©ì³ì„œ ë¡œë“œ
            if key in state_dict and tensor.shape != state_dict[key].shape:
                # lm_head.weightëŠ” ë‘ íŒŒì¼ì— ë™ì¼í•˜ê²Œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ concatí•˜ì§€ ì•ŠìŒ
                if key == "lm_head.weight":
                    # ì²« ë²ˆì§¸ ê²ƒë§Œ ì‚¬ìš©í•˜ê³  ë‘ ë²ˆì§¸ëŠ” ë¬´ì‹œ
                    if key in temp_parts:
                        continue  # ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ
                    temp_parts[key] = tensor  # ì²« ë²ˆì§¸ë§Œ ë³´ê´€
                    tensor = temp_parts.pop(key)  # ë°”ë¡œ ì‚¬ìš©
                else:
                    if key not in temp_parts:
                        temp_parts[key] = tensor
                        continue
                    else:
                        tensor = torch.cat([temp_parts.pop(key), tensor], dim=-1)

            # ë¡œë“œëœ íŒŒë¼ë¯¸í„° ëˆ„ì 
            loaded_params += tensor.numel()

            # If loading with LoRA enabled, additional padding may
            # be added to certain parameters. We only load into a
            # narrowed view of the parameter data.

            # state_dict ë”•ì…”ë„ˆë¦¬ì—ì„œ íŠ¹ì • íŒŒë¼ë¯¸í„°(key)ì— í•´ë‹¹í•˜ëŠ” í…ì„œ ê°’ì„ êº¼ëƒ„
            param_data = state_dict[key].data # íŠ¹ì • íŒŒë¼ë¯¸í„° í‚¤

            # í•´ë‹¹ íŒŒë¼ë¯¸í„°ì˜ ì „ì²´ shapeë¥¼ ê°€ì ¸ì˜´
            param_shape = state_dict[key].shape
            for dim, size in enumerate(tensor.shape):
                if size < param_shape[dim]:
                    param_data = param_data.narrow(dim, 0, size)
            if tensor.shape != param_shape:
                logger.warning(
                    "loading tensor of shape %s into "
                    "parameter '%s' of shape %s",
                    tensor.shape,
                    key,
                    param_shape,
                )

            # tensorì— ì €ì¥ëœ weight ê°’ì„ param_dataë¡œ in-place ë³µì‚¬    
            param_data.copy_(tensor)

            # state_dict ë”•ì…”ë„ˆë¦¬ì—ì„œ í˜„ì¬ key-value í•­ëª©ì„ ì œê±°
            # weightë¥¼ ë¡œë”©í•œ keyì´ë¯€ë¡œ, state_dictì´ ë‚¨ì•„ ìˆìœ¼ë©´ weightê°€ ëˆ„ë½ëœ ê²ƒ
            state_dict.pop(key)

        # ë¡œë”© ì™„ë£Œ í›„ rank ë³„ íŒŒë¼ë¯¸í„° ì¶œë ¥
        if total_params == 0:
            logger.warning(f"âœ”ï¸[Rank {rank}] No parameters to load (total_params = 0)")
        else:
            logger.info(f"âœ”ï¸[Rank {rank}] Loaded {loaded_params:,} / {total_params:,} params ({loaded_params/total_params*100:.1f}%)")


        if state_dict:
            raise ValueError(
                f"Missing keys {tuple(state_dict)} in loaded state!")

#-----------------------------------------------------------------------------------
    def iterate_over_files(
            self, paths, rank) -> Generator[tuple[str, torch.Tensor], None, None]:
        if self.runai_model_streamer:
            yield from runai_safetensors_weights_iterator(paths, True)
        else:
            from safetensors.torch import safe_open
            logger.info(f"Paths to process: {paths}")  # íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥

            for path in paths:
                logger.info(f"â˜‘ï¸[Rank {rank}] Trying to open file: {path}")
                with safe_open(path, framework="pt") as f:
                    for key in f.keys():  # noqa: SIM118
                        tensor = f.get_tensor(key)
                        yield key, tensor

#-----------------------------------------------------------------------------------
# ê¸°ì¡´ ì½”ë“œ:
# í˜„ì¬ rankê°€ ê°€ì§„ weight(state_dict)ë§Œ ê°€ì ¸ì˜´
# ê° í…ì„œ í¬ê¸°ë¥¼ í™•ì¸í•´ì„œ max_size ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
# ë¶„í• ëœ íŒŒì¼ë“¤ì„ {rank}-{part} í˜•íƒœë¡œ safetensorsë¡œ ì €ì¥

# ë³€ê²½ ì½”ë“œ:
# state_dictì˜ ê° key(íŒŒë¼ë¯¸í„°)ë³„ í…ì„œë¥¼ 1/2ë¡œ ë‚˜ëˆ„ì–´
#     â””â”€ ì²« ì ˆë°˜ â†’ rankX0, ë‘ ë²ˆì§¸ ì ˆë°˜ â†’ rankX1 íŒŒì¼ì— ì €ì¥


    @staticmethod
    def save_model(
        # model ìì²´ê°€ í˜„ì¬ rank í”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‘ ì¤‘ì¸ TP ë¶„í•  ëª¨ë¸
        model: torch.nn.Module, # í˜„ì¬ rankì˜ TP ë¶„í•  ëª¨ë¸ (í•„ìˆ˜)
        path: str, # ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ (í•„ìˆ˜)
        pattern: Optional[str] = None, # ì €ì¥ íŒŒì¼ ì´ë¦„ íŒ¨í„´ -> ìœ„ì—ì„œ ì •ì˜í•¨
        max_size: Optional[int] = None, # ê° íŒŒì¼ì˜ ìµœëŒ€ í¬ê¸°(ë°”ì´íŠ¸), ì´ˆê³¼ ì‹œ ì—¬ëŸ¬ íŒŒì¼ë¡œ ìª¼ê°¬
    ) -> None:
        from safetensors.torch import save_file
        from vllm.distributed import get_tensor_model_parallel_rank
        import torch

        if pattern is None:
            pattern = CustomLoader.DEFAULT_PATTERN

        rank = get_tensor_model_parallel_rank() # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ rank, ì €ì¥ íŒŒì¼ì— ì´ rank ê°’ì´ ì‚¬ìš©ë¨ -> íŒŒì¼ ë¶„ë¦¬ ê¸°ì¤€ì„

        # ì €ì¥ ë°˜ë³µ êµ¬ì¡° ì´ˆê¸°í™”
        part_idx = 0 # í•˜ë‚˜ì˜ rankê°€ ì €ì¥í•˜ëŠ” íŒŒì¼ ë²ˆí˜¸
        total_size = 0 # í˜„ì¬ íŒŒì¼ì— ëˆ„ì ëœ íŒŒë¼ë¯¸í„° í¬ê¸°

        # state_dict()ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì˜´ -> CustomLoader._filter_subtensors()ë¡œ í•„í„°ë§: ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œì™¸
        state_dict = CustomLoader._filter_subtensors(model.state_dict())

        # ì €ì¥ ë°˜ë³µ êµ¬ì¡° ì´ˆê¸°í™”: í˜„ì¬ íŒŒì¼ì— ì €ì¥í•  key-value í…ì„œ ë”•ì…”ë„ˆë¦¬ (2ê°œë¡œ ë¶„í• )
        state_dict_part_0: dict[str, torch.Tensor] = {}  # ì²« ë²ˆì§¸ ì ˆë°˜
        state_dict_part_1: dict[str, torch.Tensor] = {}  # ë‘ ë²ˆì§¸ ì ˆë°˜
        print("[ğŸ‘ŒğŸ‘Œ] CustomLoader save_model")

        # state_dict ìˆœíšŒí•˜ë©´ì„œ íŒŒì¼ ë¶„í•  ì €ì¥
        for key, tensor in state_dict.items():

            if key == "lm_head.weight":
                # ë‘ íŒŒì¼(0Â·1) ëª¨ë‘ì— ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
                state_dict_part_0[key] = tensor.contiguous()
                state_dict_part_1[key] = tensor.contiguous()
                param_size = tensor.nelement() * tensor.element_size()
                total_size += param_size
                continue

                if not logged_lm_head:          # í•œ rankì—ì„œ 1ë²ˆë§Œ ì¶œë ¥
                    print(f"[CHECK][rank {rank}] lm_head.weight duplicated "
                        f"â†’ will be in {rank}0 / {rank}1")
                    logged_lm_head = True
                continue

            # ê° í…ì„œë¥¼ 2ë“±ë¶„
            if len(tensor.shape) >= 1 and tensor.shape[-1] >= 2:
                # torch.splitì„ ì‚¬ìš©í•˜ì—¬ contiguousí•œ í…ì„œ ìƒì„±
                split_size = tensor.shape[-1] // 2
                remaining_size = tensor.shape[-1] - split_size
            
                # ì²« ë²ˆì§¸ ì ˆë°˜ê³¼ ë‘ ë²ˆì§¸ ì ˆë°˜ìœ¼ë¡œ ë¶„í• 
                first_half, second_half = torch.split(tensor, [split_size, remaining_size], dim=-1)
                first_half = first_half.contiguous()
                second_half = second_half.contiguous()
            
                # í…ì„œê°€ ëª‡ ë°”ì´íŠ¸ë¥¼ ì°¨ì§€í•˜ëŠ”ì§€ ê³„ì‚°
                param_size_0 = first_half.nelement() * first_half.element_size()
                param_size_1 = second_half.nelement() * second_half.element_size()
            
                # ì €ì¥í•  í…ì„œë¥¼ ì¶”ê°€í–ˆì„ ë•Œ, ì„¤ì •ëœ ìµœëŒ€ íŒŒì¼ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
                if max_size is not None and total_size + param_size_0 > max_size: # total_size: ëˆ„ì  í¬ê¸°
                
                    # ì €ì¥í•  íŒŒì¼ ì´ë¦„ì„ ìƒì„±
                    filename_0 = pattern.format(rank=f"{rank}0", part=part_idx)
                    filename_1 = pattern.format(rank=f"{rank}1", part=part_idx)
                
                    # ë”•ì…”ë„ˆë¦¬ {key: tensor}ë¥¼ .safetensors íŒŒì¼ë¡œ ì €ì¥
                    save_file( # ì§€ê¸ˆê¹Œì§€ ëª¨ì•„ë‘” íŒŒë¼ë¯¸í„° ë¬¶ìŒ (ì²« ë²ˆì§¸ ì ˆë°˜)
                        {k: v.contiguous() for k, v in state_dict_part_0.items()}, # contiguousë¡œ ë³€í™˜
                        os.path.join(path, filename_0),
                    )
                    save_file( # ì§€ê¸ˆê¹Œì§€ ëª¨ì•„ë‘” íŒŒë¼ë¯¸í„° ë¬¶ìŒ (ë‘ ë²ˆì§¸ ì ˆë°˜)
                        {k: v.contiguous() for k, v in state_dict_part_1.items()}, # contiguousë¡œ ë³€í™˜
                        os.path.join(path, filename_1),
                    )
                    print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œ] {rank}, {filename_0}")
                    print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œ] {rank}, {filename_1}")

                    # ë‹¤ìŒì— ì €ì¥í•  íŒŒì¼ì˜ part ë²ˆí˜¸ë¥¼ 1 ì¦ê°€ -> í•˜ë‚˜ì˜ rankê°€ ì—¬ëŸ¬ íŒŒì¼ì„ ì €ì¥
                    part_idx += 1
                    # ëˆ„ì  í¬ê¸°ë¥¼ ì´ˆê¸°í™”í•´ì„œ ë‹¤ìŒ íŒŒì¼ì— í…ì„œë“¤ì„ ë‹¤ì‹œ ì±„ìš°ê¸° ì‹œì‘
                    total_size = 0
                    # save_fileë¡œ ì €ì¥í•œ í…ì„œ ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ˆê¸°í™”
                    state_dict_part_0 = {}
                    state_dict_part_1 = {}

                # í˜„ì¬ ìˆœíšŒ ì¤‘ì¸ í…ì„œì˜ ì ˆë°˜ë“¤ì„ ê°ê° state_dict_partì— ì¶”ê°€
                state_dict_part_0[key] = first_half
                state_dict_part_1[key] = second_half
                # ì´ í…ì„œì˜ ë°”ì´íŠ¸ ìˆ˜ë¥¼ í˜„ì¬ íŒŒì¼ì— ëˆ„ì ëœ ìš©ëŸ‰ì— ë”í•¨
                total_size += param_size_0
            
            else:
                # ë¶„í• í•  ìˆ˜ ì—†ëŠ” í…ì„œëŠ” ë³µì œ
                tensor_contiguous = tensor.contiguous()
                param_size = tensor_contiguous.nelement() * tensor_contiguous.element_size()
            
                # ì €ì¥í•  í…ì„œë¥¼ ì¶”ê°€í–ˆì„ ë•Œ, ì„¤ì •ëœ ìµœëŒ€ íŒŒì¼ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
                if max_size is not None and total_size + param_size > max_size: # total_size: ëˆ„ì  í¬ê¸°
                
                    # ì €ì¥í•  íŒŒì¼ ì´ë¦„ì„ ìƒì„±
                    filename_0 = pattern.format(rank=f"{rank}0", part=part_idx)
                    filename_1 = pattern.format(rank=f"{rank}1", part=part_idx)
                
                    # ë”•ì…”ë„ˆë¦¬ {key: tensor}ë¥¼ .safetensors íŒŒì¼ë¡œ ì €ì¥
                    save_file(
                        state_dict_part_0, # ì§€ê¸ˆê¹Œì§€ ëª¨ì•„ë‘” íŒŒë¼ë¯¸í„° ë¬¶ìŒ (ì²« ë²ˆì§¸ ì ˆë°˜)
                        os.path.join(path, filename_0),
                    )
                    save_file(
                        state_dict_part_1, # ì§€ê¸ˆê¹Œì§€ ëª¨ì•„ë‘” íŒŒë¼ë¯¸í„° ë¬¶ìŒ (ë‘ ë²ˆì§¸ ì ˆë°˜)
                        os.path.join(path, filename_1),
                    )
                    print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œ] {rank}, {filename_0}")
                    print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œ] {rank}, {filename_1}")

                    # ë‹¤ìŒì— ì €ì¥í•  íŒŒì¼ì˜ part ë²ˆí˜¸ë¥¼ 1 ì¦ê°€ -> í•˜ë‚˜ì˜ rankê°€ ì—¬ëŸ¬ íŒŒì¼ì„ ì €ì¥
                    part_idx += 1
                    # ëˆ„ì  í¬ê¸°ë¥¼ ì´ˆê¸°í™”í•´ì„œ ë‹¤ìŒ íŒŒì¼ì— í…ì„œë“¤ì„ ë‹¤ì‹œ ì±„ìš°ê¸° ì‹œì‘
                    total_size = 0
                    # save_fileë¡œ ì €ì¥í•œ í…ì„œ ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ˆê¸°í™”
                    state_dict_part_0 = {}
                    state_dict_part_1 = {}

                # í˜„ì¬ ìˆœíšŒ ì¤‘ì¸ í…ì„œë¥¼ ì–‘ìª½ ëª¨ë‘ì— ì¶”ê°€ (ë³µì œ)
                state_dict_part_0[key] = tensor_contiguous
                state_dict_part_1[key] = tensor_contiguous
                # ì´ í…ì„œì˜ ë°”ì´íŠ¸ ìˆ˜ë¥¼ í˜„ì¬ íŒŒì¼ì— ëˆ„ì ëœ ìš©ëŸ‰ì— ë”í•¨
                total_size += param_size

        # ë°˜ë³µì´ ëë‚œ í›„, ì €ì¥ë˜ì§€ ì•Šì€ ë§ˆì§€ë§‰ ë¬¶ìŒì„ ì €ì¥   
        if len(state_dict_part_0) > 0:
            filename_0 = pattern.format(rank=f"{rank}0", part=part_idx)
            filename_1 = pattern.format(rank=f"{rank}1", part=part_idx)
            save_file(
                state_dict_part_0,
                os.path.join(path, filename_0),
            )
            save_file(
                state_dict_part_1,
                os.path.join(path, filename_1),
            )
            print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œâœ…] {rank}, {filename_0}")
            print(f"[ğŸ‘ŒğŸ‘ŒğŸ‘Œâœ…] {rank}, {filename_1}")

